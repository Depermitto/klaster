#+TITLE: Opracowanie nowoczesnych metod głębokiego uczenia
#+AUTHOR: Piotr Jabłoński 325163
#+DATE: 5 lipca 2025
#+LANGUAGE: Polish
#+OPTIONS: toc:nil
#+LATEX_HEADER: \usepackage[a4paper, margin=.7in]{geometry}
#+LATEX_HEADER: \hypersetup{colorlinks=true,linkcolor=black}

* Moduł Uczenia Reprezentacji

  Skupia się na różnych podejściach do przekształcania surowych danych w niskowymiarowe reprezentacje, które ułatwiają efektywne grupowanie. Można wyróżnić kilka kluczowych metod:

  1. *Autoenkodery*: Prosta i efektywna metoda, która uczy się reprezentacji poprzez rekonstrukcję danych wejściowych. Choć łatwa w implementacji, może ignorować relacje między instancjami, co ogranicza jej skuteczność w klastrowaniu.

  2. *Generatywne uczenie reprezentacji*: Metody takie jak VAE (Variational Autoencoder) wykorzystują modele generatywne do wnioskowania o rozkładzie danych. Są elastyczne i pozwalają na generowanie nowych danych, ale wymagają założeń dotyczących rozkładu danych.

  3. *Maksymalizacja informacji wzajemnej*: Mierzy zależność między zmiennymi, np. między różnymi warstwami sieci neuronowych. Pozwala na uczenie spójnych reprezentacji, ale może nie uwzględniać relacji między instancjami w sposób jawny.

  4. *Uczenie kontrastywne*: Polega na przyciąganiu podobnych instancji i odpychaniu różnych, co sprzyja tworzeniu dyskryminacyjnych reprezentacji. Metoda ta jest szczególnie skuteczna w zadaniach klastrowania dzięki właściwościom wyrównania i jednorodności.

  5. *Reprezentacje przyjazne klastrowaniu*: Specjalnie zaprojektowane do wspierania konkretnych metod klastrowania, np. K-means lub spektralnego. Choć skuteczne, mogą być mało uniwersalne.

  6. *Uczenie reprezentacji w podprzestrzeniach*: Wykorzystuje założenie samowyrażalności danych, ale może być kosztowne obliczeniowo dla dużych zbiorów danych.

  Bardzo istotne, aby wybrana metoda była w stanie efektywnie reprezentować wiele typów danych, takich jak obrazy, tekst, wideo czy grafy. Główne wyzwania, z którymi moduł uczenia reprezentacji się spiera to efektywne modelowanie relacji między instancjami i zapewnienie skalowalności.

* Moduł Klastrowania

  Zadaniem tego modułu jest przekształcanie niskowymiarowych reprezentacji danych w przypisania do klastrów. W przeciwieństwie do płytkich metod klastrowania, podejścia głębokie wykorzystują sieci neuronowe, aby generować miękkie przypisania probabilistyczne, unikając problemów związanych z optymalizacją dyskretnych etykiet. Istnieje kilka kluczowych metod:

  1. *Relation Matching*: Porównuje podobieństwa między instancjami w przestrzeni osadzeń i przestrzeni etykiet, aby zachować spójność semantyczną. Choć intuicyjne, wymaga obliczeń dla wszystkich par instancji, co może być kosztowne.

  2. *Pseudo Labeling*: Wykorzystuje przypisania klastrów o wysokiej pewności jako nadzór, stosując podejścia instancyjne (np. entropia) lub relacyjne (np. must-link/cannot-link). Jakość pseudoznaczników zależy jednak od wstępnej wydajności modelu.

  3. *Samouczenie*: Optymalizuje rozkład przypisań klastrów poprzez minimalizację dywergencji Kullbacka-Leiblera względem rozkładu pomocniczego. Zapobiega degeneracji rozwiązań (np. przypisaniu wszystkich instancji do jednego klastra) dzięki normalizacji zarówno instancyjnej, jak i klastrowej.

  4. *Maksymalizacja informacji wzajemnej*: Mierzy zależność między danymi a przypisaniami klastrów, łącząc zalety uczenia reprezentacji i klastrowania w spójną strukturę.

  5. *Klastrowanie kontrastywne*: Dzieli się na trzy warianty:
     - *Instancyjny*: Kontrastuje przypisania klastrów dla różnych widoków tej samej instancji.
     - *Klastrowy*: Wymusza różnorodność między centroidami klastrów.
     - *Instancyjno-klastrowy*: Przyciąga instancje do odpowiadających im centroidów, jednocześnie odpychając od innych.

  Metody te łączą zalety uczenia kontrastywnego (odporność na augmentację) z wymaganiami klastrowania, takimi jak separowalność klastrów. Należy jednak wziąć pod uwagę wyzwania, takie jak wrażliwość na zdegenerowane rozwiązania czy zależność od jakości pseudoznaczników, oraz potrzebę dalszych badań nad skalowalnością i równoważeniem między efektywnością a modelowaniem globalnych relacji.

* Taksonomia Metod Głębokiego Klastrowania

  Ten rozdział klasyfikuje metody głębokiego klastrowania w oparciu o sposób interakcji między modułem uczenia reprezentacji a modułem klastrowania. Wyróżniono cztery główne kategorie:

  1. *Wieloetapowe klastrowanie (Multi-stage)*: Metody te oddzielają proces uczenia reprezentacji od klastrowania, najpierw przekształcając dane, a następnie stosując klasyczne algorytmy (np. K-means). Choć proste w implementacji, są suboptymalne, ponieważ reprezentacje nie są specjalnie dostosowane do klastrowania, a informacja zwrotna z klastrów nie wpływa na uczenie.

  2. *Iteracyjne klastrowanie (Iterative)*: Polega na naprzemiennym aktualizowaniu reprezentacji i przypisań klastrów, wykorzystując pseudoznaczniki lub relacje jako wskazówki. Pomimo wzajemnego wzmacniania obu modułów, metody te są podatne na propagację błędów, szczególnie we wczesnych fazach uczenia.

  3. *Generatywne klastrowanie (Generative)*: Modeluje strukturę klastrów jako rozkład generatywny (np. mieszaninę Gaussa w VAE lub GAN). Pozwala to na jednoczesne uczenie reprezentacji i klastrowania, ale wymaga założeń dotyczących rozkładu danych i może być niestabilne w treningu.

  4. *Jednoczesne klastrowanie (Simultaneous)*: Optymalizuje oba moduły wspólnie w ramach jednego procesu, np. poprzez samouczenie, maksymalizację informacji wzajemnej lub uczenie kontrastywne. Choć metody te osiągają lepszą spójność, są podatne na zdegenerowane rozwiązania (np. kolaps do jednego klastra) i wymagają starannego balansowania między celami uczenia reprezentacji a klastrowania.

  Jednoczesne metody są obecnie najbardziej obiecujące, ale wciąż wymagają badań nad poprawą ich stabilności i skalowalności. Dodatkowo, hybrydowe podejścia łączące różne techniki (np. kontrast + samouczenie) pokazują potencjał w przezwyciężaniu ograniczeń poszczególnych kategorii. Kluczowym wyzwaniem pozostaje zapewnienie, aby interakcja między modułami prowadziła do wzajemnego wzmocnienia, a nie konfliktu celów optymalizacji.

* Aplikacje

  Prezentuje kluczowe zastosowania głębokiego klastrowania w różnych dziedzinach, podkreślając jego wartość w odkrywaniu ukrytych wzorców w danych:

  1. *Community Detection*: W analizie sieci społecznościowych głębokie klastrowanie pozwala identyfikować spójne grupy węzłów, łącząc topologię grafu z atrybutami węzłów. Metody te stopniowo zastępują tradycyjne podejścia oparte na modularności czy spektralnych.

  2. *Wykrywanie anomalii*: Techniki klastrowania służą do identyfikacji nietypowych wzorców, gdzie anomalie są definiowane jako punkty odległe od centroidów klastrów. Nowoczesne podejścia integrują detekcję anomalii z procesem klastrowania w jednolity framework.

  3. *Segmentacja i detekcja obiektów*: W przetwarzaniu obrazów i wideo, klastrowanie pikseli lub regionów umożliwia automatyczną segmentację scen. W przypadku danych 3D, techniki te wspierają również detekcję obiektów o spójnej geometrii.

  4. *Zastosowania medyczne*: Głębokie klastrowanie znajduje zastosowanie w automatycznej kategoryzacji obrazów medycznych oraz analizie danych sekwencjonowania RNA, wspierając diagnostykę i odkrywanie nowych typów komórek.

  Obecnie głębokie klastrowanie nabiera znaczenie w analizie złożonych, wielowymiarowych danych, gdzie tradycyjne metody często zawodzą. Szczególną uwagę zwraca się na potencjał integracji klastrowania z innymi zadaniami uczenia maszynowego, takimi jak transfer learning czy detekcja anomalii, co otwiera nowe możliwości w badaniach naukowych i zastosowaniach przemysłowych.

* Kierunki Rozwoju

  Ten rozdział identyfikuje kluczowe wyzwania i perspektywy rozwoju głębokiego klastrowania, wskazując obszary wymagające dalszych badań:

  1. *Inicjacja i stabilność*: Problemy związane z inicjacją modułu klastrowania oraz wrażliwość na warunki początkowe pozostają nierozwiązanymi wyzwaniami, pomimo postępów w pre-treningu reprezentacji.

  2. *Instancje należące do wielu klastrów*: Obecne metody głównie skupiają się na rozłącznych klastrach, podczas gdy rzeczywiste scenariusze (np. tagowanie treści) często wymagają modelowania przynależności do wielu klastrów jednocześnie.

  3. *Niezbalansowane dane*: Dominujące podejścia oparte na maksymalizacji entropii zakładają równomierny rozkład klastrów, co ogranicza ich skuteczność w przypadku danych o nierównomiernej strukturze.

  4. *Integracja z uczeniem reprezentacji*: Istnieje potrzeba lepszego wykorzystania struktury klastrów do poprawy jakości reprezentacji, a nie tylko odwrotnego podejścia.

  5. *Interpretowalność wyników*: Brak mechanizmów wyjaśniających decyzje klastrowania utrudnia zaufanie i praktyczne wdrożenie w krytycznych zastosowaniach.

  6. *Transfer learning*: Połączenie technik adaptacji dziedzinowej z klastrowaniem oferuje obiecującą ścieżkę dla problemów z ograniczonymi danymi treningowymi.

  7. *Odporność na anomalie*: Obecne metody są często wrażliwe na obecność punktów odstających, co wymaga rozwoju bardziej robustnych rozwiązań.

  8. *Skalowalność*: Konflikt między efektywnością obliczeniową a zdolnością do modelowania globalnych relacji w dużych zbiorach danych pozostaje nierozwiązany.

  Przyszły postęp w dziedzinie będzie wymagał holistycznego podejścia, łączącego teoretyczne podstawy klastrowania z najnowszymi osiągnięciami w dziedzinie głębokiego uczenia, szczególnie w kontekście rosnącej złożoności danych i wymagań aplikacyjnych.

* Klaster
  Obecnie (lipiec 2025 roku) najbardziej nowatorskim podejściem w dziedzinie głębokiego klastrowania jest *jednoczesne klastrowanie z hybrydowymi technikami* optymalizującymi zarówno reprezentacje, jak i moduł klastrowania. Celem pracy jest natomiast stworzenie biblioteki implementującej różne algorytmy grupowania, która zapewni dobre wyniki na różnorodnych danych. Problemem przy opracowywaniu nowego rozwiązania jest ryzyko, że będzie ono działało gorzej niż istniejące, sprawdzone algorytmy. Pomysły:
** Iteracyjnie (DEC-like)
*** Krok A: Forward pass + przypisanie do klastrów
    1. Dane → Encoder → Embeddingi.
    2. Oblicz podobieństwo embeddingów do centroidów klastrów (np. rozkład t-SNE lub miara odległości).
    3. Przypisz punkty do klastrów (np. metodą soft-assignment w DEC).
*** Krok B: Obliczenie straty
    Strata to np. dywergencja KL między rozkładem przypisań a celem (np. "ostre" przypisania z poprzedniej iteracji).
*** Krok C: Backpropagation + aktualizacja centroidów
    - Gradient z funkcji straty klastrowania jest propagowany wstecz przez encoder.
    - Tylko encoder jest aktualizowany – celem jest dostosowanie reprezentacji, aby lepiej pasowały do klastrów.
    - Centroidy klastrów mogą być też aktualizowane (np. K-means-like tzn. średnie embeddingów).
** Jednoczesne (simultaneous/joint)
   Przebiega podobnie co do podejścia iteracyjnego z tą różnicą, że moduł reprezentacji (np. autoenkoder) oraz moduł klastrowania (np. sieć neuronowa z wyjściem softmax albo zmodyfikowane podejście z użyciem K-means) są uczone jednocześnie, tzn. za pomocą tej samej funkcji straty. Ta funkcja zwykle sumą innych strat np.:
   - *Strata klastrowania*: np. dywergencja KL między rozkładem przypisań a celem (np. "ostre" przypisania z poprzedniej iteracji).
   - *Strata rekonstrukcji* (jeśli jest decoder): MSE(*X*, *X_recon*).
   - *Dodatkowa regularyzacja*: np. contrastive loss
   - *Całkowita strata*: `α * leoss_kluster + β * loss_recon + γ * loss_contrastive` (wagi α, β i γ balansują wpływ komponentów).
** +Propozycja algorytmu (DFC-like)+
*** +Moduł reprezentacji+
    - +Autoencoder/MLP+
    - +Inicjalizacja K-means (potencjalnie zmodyfikowane K-means z integracją funkcji straty)+
*** +Moduł klastrowania ClusterLayer+
    - +Sieć neuronowa klastrująca/Deep K-means+
*** +Funkcja straty+
    - +Dywergencja KL + MSE + loss_contrastive+
*** +Nowości+
    - +CPU-friendly. Stworzyć nowy algorytm jako część biblioteki i oczekiwać, że będzie lepszy niż istniejące state-of-the-art rozwiązania jest nierealistyczne, dlatego proponuję zaprojektować model z myślą o uruchamianiu go na procesorach. Efektywność będzie ograniczona, jednak dalej powinna być dużo lepsza niż naiwne K-means - szczególnie dla danych super-wielowymiarowych.+
    - +Optimal Transport - teoria matematyczna, która pozwala na znalezienie najefektywniejszego sposobu przenoszenia "masy" między rozkładami prawdopodobieństwa przy minimalnym koszcie. Można użyć w module klastrowania zamiast funkcji softmax, albo K-means. Można spróbować użyć zamiast embeddingów w przestrzeni cech.+
** +DONE Prototyp+
   +DEADLINE: <2025-07-14 Mon 11:30>+
*** +Implementacja w Python+
*** +Problem: sprawdzić czy koncepcja jest dobra na danych wielowymiarowych (mój vs K-means)+
** Problem: Rozpoznawanie odręcznie pisanych znaków (klastrowanie obrazów)
*** Proces klastrowania
    - Konwersja przestrzeni kolorów na odcienie szarości (grayscale)
    - (Opcjonalnie) Użycie STN, aby uzbroić się przed transformacjami obrazu
    - Uczenie podobnie do NDDC_v1
